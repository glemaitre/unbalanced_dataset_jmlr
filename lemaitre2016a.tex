\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pifont} 

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\jmlrheading{17}{2016}{1-5}{7/16; Revised 7/16}{11/2016}{Guillaume Lema\^itre, Fernando Nogueira, 
%Dayvid V. R. Oliveira - removing my name.
and Christos K. Aridas}

% Short headings should be running head and authors last names
\ShortHeadings{A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning}{Lema\^itre, Nogueira, and Aridas}
\firstpageno{1}

\begin{document}

\title{Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning}
\author{Guillaume Lema\^itre \email g.lemaitre58@gmail.com \\ 
    \addr{LE2I UMR6306, CNRS, Arts et M\'etiers, Univ. Bourgogne Franche-Comt\'e} \\ 
    \addr{12 rue de la Fonderie, 71200 Le Creusot, France} \\ 
    \addr{ViCOROB, Universitat de Girona} \\ 
    \addr{Campus Montilivi, Edifici P4, 17071 Girona, Spain}
        \AND
        Fernando Nogueira \email fmfnogueira@gmail.com \\ 
        \addr{ShoppeAI} \\ 
        \addr{488 Wellington Street West, Suite 304, Toronto, Ontario M5V 1E3, Canada}
        % REMOVING MY NAME AS COAUTHOR
        %\AND
        %Dayvid V. R. Oliveira \email dvro@cin.ufpe.br \\ 
        %\addr{VIISAR Research Group, Centro de Inform\'atica - Universidade Federal de Pernambuco} \\ 
        %%\addr{Av. Prof. Luiz Freire, s/n - Cidade Universitária Recife - PE, 50740-540, Brazil}} 
        %\addr{Av. Jornalista Aníbal Fernandes, s/n - Cidade Universitária - PE, 50740-560, Brazil}
        \AND
        Christos K. Aridas \email char@upatras.gr \\ 
        \addr{Computational Intelligence Laboratory} \\
        \addr{Department of Mathematics} \\ 
        \addr{University of Patras} \\ 
        \addr{GR-26504 Patras, Greece}} 
\editor{Geoff Holmes}

\maketitle

\begin{abstract}%
\texttt{imbalanced-learn} is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition.
The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods.
The proposed toolbox only depends on \texttt{numpy}, \texttt{scipy}, and \texttt{scikit-learn} and is distributed under MIT license.
Furthermore, it is fully compatible with \texttt{scikit-learn} and is part of the \texttt{scikit-learn-contrib} supported project.
Documentation, unit tests as well as integration tests are provided to ease usage and contribution.
The toolbox is publicly available in GitHub \\ \url{https://github.com/scikit-learn-contrib/imbalanced-learn}.
\end{abstract}

\begin{keywords}
Imbalanced Dataset, Over-Sampling, Under-Sampling, Ensemble Learning, Machine Learning, Python.
\end{keywords}

\section{Introduction}

Real world datasets commonly show the particularity to have a number of samples of a given class under-represented compared to other classes.
This imbalance gives rise to the ``class imbalance'' problem~\citep{prati2009data} (or ``curse of imbalanced datasets'')
which is the problem of learning a concept from the class that has a small number of samples.

The class imbalance problem has been encountered in multiple areas such as 
telecommunication managements, bioinformatics, fraud detection, and medical diagnosis,
and has been considered one of the top 10 problems in data mining and 
pattern recognition~\citep{yang200610,rastgoo2016tackling}. 
Imbalanced data substantially compromises the learning process, since most of the 
standard machine learning algorithms expect balanced class distribution or an 
equal misclassification cost~\citep{he2009learning}. For this reason, several
approaches have been specifically proposed to handle such datasets.
Such standalone methods have been implemented mainly in R language~\citep{torgo2010data,kuhn2015caret,dal2013racing}.
Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available~\citep{pedregosa2011scikit,sonnenburg2010shogun}.

In this paper, we present the \texttt{imbalanced-learn} API, 
\textit{a python toolbox to tackle the curse of imbalanced datasets
in machine learning}. The following sections present the project vision, a snapshot of the API, an overview of the implemented methods,
and finally, the conclusion of this paper, including future functionalities
for the \texttt{imbalanced-learn} API.

\section{Project management}

\noindent\textit{Quality assurance} In order to ensure code quality, a set of unit tests is provided leading to a coverage of 99 \% for the release 0.1.8 of the toolbox.
Furthermore, the code consistency is ensured by following \texttt{PEP8} standards and each new contribution is automatically checked through landscape, which provides metrics related to code quality.

\noindent\textit{Continuous integration} To allow user and developer to either use or contribute to this toolbox, Travis CI is used to easily integrate new code and ensure back-compatibility.

\noindent\textit{Community-based development} All the development is performed in a collaborative manner.
Tools such as git, GitHub, and gitter are used to ease collaborative programming, issue tracking, code integration, and idea discussions.

\noindent\textit{Documentation} A consistent API documentation is provided using \texttt{sphinx} and \texttt{numpydoc}.
An additional installation guide and examples are also provided and centralized on GitHub\footnote{\url{https://github.com/scikit-learn-contrib/imbalanced-learn}}.

\noindent\textit{Project relevance} At the edition time, the repository is visited no less than $2,000$ per week, attracting about $300$ unique visitors per week.
Additionally, the toolbox is supported by \texttt{scikit-learn} through the \texttt{scikit-learn-contrib} projects.

\section{Implementation design}

\begin{lstlisting}[language=Python, caption=Code snippet to over-sample a dataset using SMOTE.]
from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# Generate the dataset
X, y = make_classification(n_classes=2, weights=[0.1, 0.9],
                           n_features=20, n_samples=5000)

# Apply the SMOTE over-sampling
sm = SMOTE(ratio='auto', kind='regular')
X_resampled, y_resampled = sm.fit_sample(X, y)
\end{lstlisting}

The implementation relies on \texttt{numpy}, \texttt{scipy}, and \texttt{scikit-learn}.
Each sampler class implements three main methods inspired from the \texttt{scikit-learn} API:
(i) \texttt{fit} computes the parameter values which are later needed to resample the data into a balanced set;
(ii) \texttt{sample} performs the sampling and returns the data with the desired balancing ratio;
and (iii) \texttt{fit\_sample} is equivalent to calling the method \texttt{fit} followed by the method \texttt{sample}.
A class \texttt{Pipeline} is inherited from \texttt{scikit-learn} toolbox to automatically combine \texttt{samplers}, \texttt{transformers}, and \texttt{estimators}.

\section{Implemented methods}

The \texttt{imbalanced-learn} toolbox provides four different strategies to tackle the problem of imbalanced dataset:
(i) under-sampling, (ii) over-sampling, (iii) a combination of both, and (iv) ensemble learning.
The following subsections give an overview of the techniques implemented.


\begin{table}
  \centering
  \scriptsize
   \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \multicolumn{2}{c}{\textbf{Over-sampling}} & \multicolumn{2}{c}{\textbf{Under-sampling}} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    & Binary & Mutli-class & Binary & Multiclass \\
    \midrule
    ADASYN~\citep{he2008adasyn} & \cmark & \xmark & \xmark & \xmark \\
    SMOTE~\citep{chawla2002smote,han2005borderline,nguyen2011borderline} & \cmark & \xmark & \xmark & \xmark \\
    ROS & \cmark & \cmark & \xmark & \xmark \\
    CC & \xmark & \xmark & \cmark & \cmark \\
    CNN~\citep{hart1968condensed} & \xmark & \xmark & \cmark & \cmark \\
    ENN~\citep{wilson1972asymptotic} & \xmark & \xmark & \cmark & \cmark \\
    RENN & \xmark & \xmark & \cmark & \cmark \\
    AKNN & \xmark & \xmark & \cmark & \cmark \\
    NM~\citep{mani2003knn} & \xmark & \xmark & \cmark & \cmark \\
    NCL~\citep{laurikkala2001improving} & \xmark & \xmark & \cmark & \cmark \\
    OSS~\citep{kubat1997addressing} & \xmark & \xmark & \cmark & \cmark \\
    RUS & \xmark & \xmark & \cmark & \cmark \\
    IHT~\citep{smith2014instance} & \xmark & \xmark & \cmark & \xmark \\
    TL~\citep{tomek1976two} & \xmark & \xmark & \cmark & \xmark \\
    BC~\citep{liu2009exploratory} & \xmark & \xmark & \cmark & \xmark \\
    EE~\citep{liu2009exploratory} & \xmark & \xmark & \cmark & \cmark \\
    SMOTE + ENN~\citep{batista2003balancing} & \cmark & \xmark & \cmark & \xmark \\
    SMOTE + TL~\citep{batista2003balancing} & \cmark & \xmark & \cmark & \xmark \\
    \bottomrule
  \end{tabular}
  \label{tab:imbmethods}
\end{table}

\subsection{Notation and background}

Let $\chi$ be an imbalanced dataset with $\chi_{min}$ and $\chi_{maj}$ being the subset of samples belonging to the minority and majority class, respectively.
The balancing ratio of the dataset $\chi$ is defined as:

\begin{equation}
r_{\chi} = \frac{|\chi_{min}|}{|\chi_{maj}|} \ ,
\end{equation}

\noindent where $|\cdot|$ denotes the cardinality of a set. The balancing process is equivalent to resample $\chi$ into a new dataset $\chi_{res}$ such that $r_{\chi} > r_{\chi_{res}}$.

\paragraph{Under-sampling}
Under-sampling refers to the process of reducing the number of samples in $\chi_{maj}$.
The implemented methods can be categorized into 2 groups: (i) fixed under-sampling and (ii) cleaning under-sampling.

\emph{Fixed under-sampling} refer to the methods which perform under-sampling to obtain the appropriate balancing ratio $r_{\chi_{res}}$.
%The implemented methods perform the under-sampling based on different criteria such as: (i) random selection, (ii) clustering, (iii) nearest neighbours rule (i.e., \texttt{NearMiss}~\citep{mani2003knn}), and (iv) classification accuracy (i.e., \texttt{instance hardness threshold}~\citep{smith2014instance}).

Contrary to the previous methods, \emph{cleaning under-sampling} do not allow to reach specifically the balancing ratio $r_{\chi_{res}}$, but rather clean the feature space based on some empirical criteria.
%These criteria are derived from the nearest neighbours rule, namely: (i) \texttt{condensed nearest neighbours}~\citep{hart1968condensed}, (ii) \texttt{edited nearest neighbours}~\citep{wilson1972asymptotic}, (iii) \texttt{one-sided selection}~\citep{kubat1997addressing}, (iv) \texttt{neighbourhood cleaning rule}~\citep{laurikkala2001improving}, and (v) \texttt{Tomek links}~\citep{tomek1976two}.

% Under-sampling refers to the process of reducing the number of samples in $\chi_{maj}$ to obtain the appropriate balancing ratio $r_{\chi_{res}}$.
% The following methods are considered to perform such balancing.

% \texttt{Random under-sampling} is performed by randomly selecting without replacement a subset of samples from $\chi_{maj}$ to obtain the desired balancing ratio $r_{\chi_{res}}$.

% \texttt{Cluster centroids method} refers to the use of a $k$-means to cluster the feature space.
% $k$ corresponds to the number of samples in $\chi_{res_{maj}}$ defined by the desired balancing ratio $r_{\chi_{res}}$.
% Note that the samples in $\chi_{res_{maj}}$ do not correspond to the original samples of $\chi_{maj}$ and are synthetically generated.

% \texttt{Condensed nearest neighbours} \citep{hart1968condensed}

% \texttt{Edited nearest neighbours} \citep{wilson1972asymptotic}

% \texttt{Instance hardness threshold} \citep{smith2014instance}

% \texttt{NearMiss} offers three different methods to under-sample the majority class~\citep{mani2003knn}.
% In NearMiss-1, samples $\chi_{maj}$ are selected such that for each sample, the average distance to the $k$ nearest neighbour samples from $\chi_{min}$ is minimum.
% NearMiss-2 diverges from NearMiss-1 by considering the $k$ farthest neighbours samples from $\chi_{min}$.
% In NearMiss-3, a subset $M$ containing samples from the $\chi_{maj}$ is generated by finding the $m$ nearest neighbours from each sample of $\chi_{min}$.
% Then, samples from the subset $M$ are selected such that for each sample, the average distance to the $k$ nearest neighbour samples from $\chi_{min}$ is maximum.

% \texttt{One-sided selection} \citep{kubat1997addressing}

% \texttt{Neighbourhood cleaning rule} consists of applying two rules depending on the class of each sample~\citep{laurikkala2001improving}.
% Let define $x_i$ as a sample of the dataset with its associated class label $y_i$.
% Let define $y_m$ as the class of the majority vote of the $k$ nearest neighbours of the sample $x_i$.
% If $y_i$ corresponds to $\chi_{maj}$ and $y_i \neq y_m$, $x_i$ is rejected from the final subset.
% If $y_i$ corresponds to $\chi_{min}$ class and and $y_i \neq y_m$, then the $k$ nearest neighbours are rejected from the final subset.

% \texttt{Tomek links} can be used to under-sample $\chi_{maj}$~\citep{tomek1976two}.
% Let define a pair of nearest neighbour samples $(x_i, x_j)$ such that their associated class label $y_i \neq y_j$.
% The pair $(x_i, x_j)$ is defined as a Tomek link if, by relaxing the class label differentiation constraint; there is no other sample $x_k$ defined as the nearest neighbour of either $x_i$ or $x_j$.
% Under-sampling is performed by removing the samples belonging to $\chi_{maj}$ and forming a Tomek link.

\paragraph{Over-sampling}
Contrary to under-sampling, data balancing can be performed by over-sampling such that new samples are generated in $\chi_{min}$ to reach the balancing ratio $r_{\chi_{res}}$.
%Two methods are currently available: (i) \texttt{Random over-sampling} is performed by randomly replicating the samples of $\chi_{min}$ to obtain the appropriate balancing ratio $r_{\chi_{res}}$ and \texttt{SMOTE} which randomly generate new samples between tuple of nearest neighbours of $\chi_{min}$~\citep{chawla2002smote}.
%Different variants of this algorithm have been proposed: \texttt{SMOTE borderline 1 \& 2}~\citep{han2005borderline} and \texttt{SMOTE SVM}~\citep{nguyen2011borderline}.


% \texttt{SMOTE} is a method to generate synthetic samples in the feature space~\citep{chawla2002smote}.
% Let define $x_i$ as a sample belonging to the minority class.
% Let define $x_{nn}$ as a randomly selected sample from the $k$ nearest neighbours of $x_i$, with $k$ set to 3.
% Therefore, a new sample $x_j$ is generated such that $x_j = x_i + \sigma \left( x_{nn} - x_i \right)$, where $\sigma$ is a random number in the interval $\left[0,1\right]$.
% Some variants of this algorithm have been proposed: \texttt{SMOTE borderline 1 \& 2}~\citep{han2005borderline} and \texttt{SMOTE SVM}~\citep{nguyen2011borderline}.

\paragraph{Combination of over- and under-sampling}
Over-sampling can lead to over-fitting which can be avoided by applying cleaning under-sampling methods~\citep{prati2009data}.

%\texttt{SMOTE} over-sampling can lead to over-fitting which can be avoided by applying cleaning under-sampling methods~\citep{prati2009data}.
%In that regard, \cite{batista2003balancing} combined \texttt{SMOTE} either with \texttt{Tomek links} or \texttt{edited nearest neighbours}.

\paragraph{Ensemble learning}
Under-sampling methods imply that samples of the majority class are lost during the balancing procedure.
Ensemble methods offer an alternative to use most of the samples.
In fact, an ensemble of balanced sets is created and used to later train any classifier.
%Two methods are available to build such ensemble proposed by~\cite{liu2009exploratory}: \texttt{EasyEnsemble} and \texttt{BalanceCascade}.
%The former is based on iteratively applying the \texttt{random under-sampling} method to build several sets, each of them with a desired balancing ratio $r_{\chi_{res}}$.
%The latter differs such that a classifier is used at each iteration to determine the class of the randomly selected samples.
%Misclassified samples are kept and propagated in the next subset.


% \begin{figure}
%   \centering
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:clustercentroids}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_cluster_centroids_001.png}} \hfill
%   \subfigure[]{\label{subfig:condensednearestneighbour}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_condensed_nearest_neighbour_001.png}} \hfill
%   \subfigure[]{\label{subfig:editedneasrestneighbours}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_edited_nearest_neighbours_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:nearmiss1}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_1_001.png}} \hfill
%   \subfigure[]{\label{subfig:nearmiss2}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_2_001.png}} \hfill
%   \subfigure[]{\label{subfig:nearmiss3}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_3_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:onesidedselection}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_one_sided_selection_001.png}} \hfill
%   \subfigure[]{\label{subfig:neighbourhoodcleaningrule}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_neighbourhood_cleaning_rule_001.png}} \hfill
%   \subfigure[]{\label{subfig:tomeklinks}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_tomek_links_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:smote}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteborderline1}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_bordeline_1_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteborderline2}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_bordeline_2_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:smotesvm}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_svm_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteenn}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_enn_001.png}} \hfill
%   \subfigure[]{\label{subfig:smotetomek}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_tomek_001.png}}
%   \hspace*{\fill}
%   \caption{Toy examples for the different sampling methods.}
%   \label{fig:methods}
% \end{figure}

\section{Future plans and conclusion}

In this paper, we shortly presented the foundations of the \texttt{imbalanced-learn} toolbox vision and API.
As avenues for future works, additional methods based on prototype/instance selection, generation, and reduction will be added as well as additional user guides.

% \acks{We would like to acknowledge support for this project
% from git, GitHub, Travis CI, and Gitter.}

\bibliography{bibtex}


\end{document}
